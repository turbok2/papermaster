import streamlit as st
from utils import print_messages, StreamHandler
from langchain_core.messages import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
import os
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.chains import RetrievalQA
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.chat_models import ChatOpenAI
import tempfile


# Assuming you have a chain setup
def log_step_output(step_result):
    print("Step output:", step_result)
    return step_result

# Suppose we want to use this vectorstore to find documents similar to a user's query
def get_similar_documents(query):
    # This example assumes you need to first convert the query into an embedding
    embedding = vectorstore.embedding_function.embed_documents([query])[0]
    similar_docs = vectorstore.similarity_search(embedding)
    return similar_docs
    
# Assuming we have a proper retriever method that gets the necessary context
def prepare_context(query):
    docs = get_similar_documents(query)
    context_text = ' '.join([doc['text'] for doc in docs])  # Simplified example
    return context_text
    
varRagDir="db11"
rag_flag = False
st.set_page_config(page_title="ChatGPT with Paper", page_icon="ğŸ¦œ")
st.title("ğŸ¦œ ChatGPT with Paper  ğŸ“" )

# API KEY ì„¤ì •
os.environ['OPENAI_API_KEY'] = st.secrets["OPENAI_API_KEY"]

if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ì²´íŒ… ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” store ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜
if "store" not in st.session_state:
    st.session_state["store"] = dict()

# RAG êµ¬í˜„
def save_pdf_to_db(uploaded_file):
    # varRagDir="db11"
    vectorstore = Chroma(persist_directory = varRagDir  , 
            embedding_function = 
            OpenAIEmbeddings()
            )
    print(vectorstore._collection.count())
    file_name = uploaded_file.name
    print(file_name)
    tmp_name = os.path.join('./tmp',file_name)
    result1 = vectorstore.get(where={"source": tmp_name})  # AIë°”ìš°ì³ ê´€ë ¨ ë‚´ìš©
    print("ì‚­ì œì „ ì»¬ëŸ¼ìˆ˜ : " , len(result1))
    print("ì‚­ì œì „ ë¦¬ìŠ¤íŠ¸ ê±´ìˆ˜: ", len(result1['ids']) )
    # ë°ì´í„° ì‚­ì œ 
    # https://python.langchain.com/docs/integrations/vectorstores/chroma#update-and-delete
    print("ë°ì´í„° ì‚­ì œ ì‹œì‘!!!!")
    for i in range(0, len(result1['ids']) ):
        print(i)
        iNum = result1['ids'][i]
        vectorstore._collection.delete(ids=iNum)
        
    # íŒŒì¼ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
    # tmp_name = os.path.join('./tmp',file_name)
    with open(tmp_name, 'wb') as file:
        file.write(uploaded_file.getvalue())
    
    # ëŒ€ìƒë¬¸ì„œ RAGì— ì €ì¥ 
     # ì„ì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ PyPDFLoader ì´ˆê¸°í™”
    print(tmp_name)
    loader = PyPDFLoader(tmp_name)   
    data = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)
    all_splits = text_splitter.split_documents(data)
    # print('all_splits[0]')
    # print(all_splits[0])
    # print('all_splits: ',len(all_splits))

    vectorstore = Chroma.from_documents(documents=all_splits, embedding = OpenAIEmbeddings(), 
            persist_directory=varRagDir)

    # retriever = vectorstore.as_retriever()
    
    #  ì €ì¥ëœ ë¬¸ì„œê°€ ìˆëŠ”ì§€ ê²€ìƒ‰
    result3 = vectorstore.get(where={"source": tmp_name})  # AIë°”ìš°ì³ ê´€ë ¨ ë‚´ìš©
    print("ì €ì¥í›„ ì»¬ëŸ¼ìˆ˜ : " , len(result3))
    print("ì €ì¥í›„ ë¦¬ìŠ¤íŠ¸ ê±´ìˆ˜: ", len(result3['ids']) )
    # print(result3)
    
    # ì‚¬ìš© í›„ ì„ì‹œ íŒŒì¼ ì‚­ì œ (í•„ìš” ì‹œ ì£¼ì„ ì²˜ë¦¬)
    os.remove(tmp_name)    
    
    # return retriever
    return vectorstore
    

with st.sidebar:
    #íŒŒì¼ ì—…ë¡œë“œ
    st.subheader('ğŸ“ ë…¼ë¬¸ íŒŒì¼ ì—…ë¡œë“œ')
    uploaded_file = st.file_uploader('íŒŒì¼ ì„ íƒ')

    if uploaded_file is not None:
        vectorstore = save_pdf_to_db(uploaded_file)
        st.write('ë…¼ë¬¸ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ.')
        st.info(uploaded_file.name)
    else:
        st.write('ë…¼ë¬¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.')
        vectorstore = Chroma(persist_directory=varRagDir, embedding_function=OpenAIEmbeddings())
    retriever = vectorstore.as_retriever()
    st.write('---')
    if st.checkbox('Using RAG'):
        rag_flag = True    
        st.info('ì½ì–´ë‘” ë…¼ë¬¸ pdf ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.')
    st.write('---')
    session_id =  st.text_input("Session_ID", value='abc123')

    clear_btn = st.button("ëŒ€íšŒê¸°ë¡ ì´ˆê¸°í™”")    
    if clear_btn:
        st.session_state["messages"] = []
        st.experimental_rerun()
# ì´ì „ ëŒ€í™”ê¸°ë¡ì„ ì¶œë ¥í•´ì£¼ëŠ” ì½”ë“œ
print_messages()


# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_ids: str) -> BaseChatMessageHistory:
    if session_ids not in st.session_state["store"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
        st.session_state["store"][session_ids] = ChatMessageHistory()
    return st.session_state["store"][session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜



if user_input := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}") 
    # st.session_state["messages"].append(("user",user_input))
    st.session_state["messages"].append(ChatMessage(role="user", content=user_input))
    # LMMì„ ì‚¬ìš©í•˜ì—¬ AIì˜ ë‹µë³€ì„ ìƒì„±

    # AIì˜ ë‹µë³€
    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())

        # 1. ëª¨ë¸ ìƒì„±
        llm = ChatOpenAI(streaming=True, callbacks=[stream_handler],
                         model_name="gpt-3.5-turbo", 
                        temperature=0)
#í•™ìŠµëœ ë¬¸ì„œë‚´ì—ì„œ ë‹µë³€í•´ ì£¼ì„¸ìš”. í•™ìŠµëœ ë¬¸ì„œë‚´ì˜ ì§ˆë¬¸ì´ ì•„ë‹ˆë©´ 'ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ì€ ì´ ë…¼ë¬¸ê³¼ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤. 'ë¼ê³  ëŒ€ë‹µí•´ ì£¼ì„¸ìš”.
        if rag_flag:
            # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
            template = """ 
            í•™ìŠµëœ ë¬¸ì„œë‚´ì—ì„œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
            {context}
            Question: {question}
            Answer:"""        
            prompt = PromptTemplate.from_template(template)
            chain = ( #prompt | llm
                {"context":  RunnablePassthrough(), "question": RunnablePassthrough()} 
                # {"context":  retriever, "question": RunnablePassthrough()} 
                # | (prompt|log_step_output) # debug
                | prompt
                # |  (llm|log_step_output) # debug
                |  llm

            )
            # question="AIë°”ìš°ì²˜ê°€ ë­ì˜ˆìš”?"
            # result = chain.invoke(question).content   
            # print(result)
            docs = retriever.get_relevant_documents(user_input)
            print(len(docs))
            # print( docs[0])

            
            chain_with_memory = (
                RunnableWithMessageHistory(  # RunnableWithMessageHistory ê°ì²´ ìƒì„±
                    chain,  # ì‹¤í–‰í•  Runnable ê°ì²´
                    get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
                    input_messages_key="question",  # ì‚¬ìš©ì ì§ˆë¬¸ì˜ í‚¤
                    history_messages_key="history",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
                )
            )
            # print('chain_with_memory: ')
            # print(chain_with_memory)
            
            response = chain_with_memory.invoke(
                # {"question": user_input},
                {"context":  docs, "question": user_input},
                # ì„¸ì…˜ ID ì„¤ì •
                config={"configurable": {"session_id": session_id}},
            )
            # print('response: ')
            # print(response)
            # print("Response from the model:", response.content)
        else:
            # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        "ë‹¹ì‹ ì€ ëŒ€í•™êµìˆ˜ë¡œ í•™ìƒë“¤ì˜ ë…¼ë¬¸ì§€ë„ë¥¼ 50ë…„ ì´ìƒ í•´ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. ëŒ€í•™ì›ìƒë“¤ì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.",
                    ),
                    # ëŒ€í™” ê¸°ë¡ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©, history ê°€ MessageHistory ì˜ key ê°€ ë¨
                    MessagesPlaceholder(variable_name="history"),
                    ("human", "{question}"),  # ì‚¬ìš©ì ì§ˆë¬¸ì„ ì…ë ¥
                ]
            )
            chain = prompt | llm  # í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ì„ ì—°ê²°í•˜ì—¬ runnable  ê°ì²´ ìƒì„±
            chain_with_memory = (
                RunnableWithMessageHistory(  # RunnableWithMessageHistory ê°ì²´ ìƒì„±
                    chain,  # ì‹¤í–‰í•  Runnable ê°ì²´
                    get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
                    input_messages_key="question",  # ì‚¬ìš©ì ì§ˆë¬¸ì˜ í‚¤
                    history_messages_key="history",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
                )
            )

            response = chain_with_memory.invoke(
                {"question": user_input},
                # ì„¸ì…˜ ID ì„¤ì •
                config={"configurable": {"session_id": session_id}},
            )

        msg = response.content    

        st.session_state["messages"].append(ChatMessage(role="assistant", content=response.content))

